import numpy as np
import numpy.ma as ma
import pandas as pd
import tensorflow as tf
import tabulate 

from tensorflow import keras
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.model_selection import train_test_split



user_train = pd.read_csv('user_train.csv')
item_train = pd.read_csv('item_train.csv')


user_train.head(10)





item_train.head(10)





cols = item_train.columns.tolist()
cols[0], cols[1] = cols[1], cols[0]
item_train = item_train[cols]


item_train.head(10)


y_train = user_train['overall_score']



type(y_train)




item = item_train
item_train = item_train.drop(['kdrama_id', 'drama_name'],axis = 1)


user = user_train
user_train = user_train.drop(['user_id', 'overall_score'], axis =1)






item_train_unscaled = item_train
user_train_unscaled = user_train
y_train_unscaled    = y_train

scalerItem = StandardScaler()
scalerItem.fit(item_train)
item_train = scalerItem.transform(item_train)

scalerUser = StandardScaler()
scalerUser.fit(user_train)
user_train = scalerUser.transform(user_train)

scalerTarget = MinMaxScaler((-1, 1))
scalerTarget.fit(y_train.values.reshape(-1, 1))
y_train = scalerTarget.transform(y_train.values.reshape(-1, 1))





print(np.allclose(item_train_unscaled, scalerItem.inverse_transform(item_train)))
print(np.allclose(user_train_unscaled, scalerUser.inverse_transform(user_train)))


item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)
user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)
y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)
print(f"movie/item training data shape: {item_train.shape}")
print(f"movie/item test data shape: {item_test.shape}")


num_user_features = user_train.shape[1]
num_item_features = item_train.shape[1]


num_outputs = 32
tf.random.set_seed(1)
user_NN = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation = 'relu'),
    tf.keras.layers.Dense(128, activation = 'relu'),
    tf.keras.layers.Dense(num_outputs, activation = 'linear'),
  
  
])

item_NN = tf.keras.models.Sequential([
    tf.keras.layers.Dense(256, activation = 'relu'),
    tf.keras.layers.Dense(128, activation = 'relu'),
    tf.keras.layers.Dense(num_outputs, activation = 'linear'),
  
  
])

input_user = tf.keras.layers.Input(shape=(num_user_features))
vu = user_NN(input_user)
vu = tf.linalg.l2_normalize(vu, axis=1)

input_item = tf.keras.layers.Input(shape=(num_item_features))
vm = item_NN(input_item)
vm = tf.linalg.l2_normalize(vm, axis=1)

output = tf.keras.layers.Dot(axes=1)([vu, vm])

model = tf.keras.Model([input_user, input_item], output)

model.summary()


tf.random.set_seed(1)
cost_fn = tf.keras.losses.MeanSquaredError()
opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.01)
model.compile(optimizer=opt,
              loss=cost_fn)


tf.random.set_seed(1)
model.fit([user_train, item_train], y_train, epochs=100)


model.evaluate([user_test, item_test], y_test)





user.head()


uid = 0
user_vecs = user[user['user_id']== uid]

y_vecs = user_vecs['overall_score']
user_vecs = user_vecs.drop(['user_id', 'overall_score'], axis = 1)
y_vecs.head(30)


user_vecs.head(30)


item.head()


index_u = user_vecs.index.tolist()
item_vecs = item.iloc[index_u[0]:index_u[-1]+1,2:]
item_vecs.head(23)







# scale our user and item vectors
suser_vecs = scalerUser.transform(user_vecs)
sitem_vecs = scalerItem.transform(item_vecs)

# make a prediction
y_p = model.predict([suser_vecs, sitem_vecs])

# unscale y prediction 
y_pu = scalerTarget.inverse_transform(y_p)


#


print(y_pu)


# sort the results, highest prediction first
sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  
sorted_ypu   = y_pu[sorted_index]
v = item[['kdrama_id','drama_name']].iloc[index_u[0]:index_u[-1]+1,:]
sorted_items = v.reset_index(drop= True).iloc[sorted_index]
sorted_user  = user.iloc[sorted_index]


sorted_y = y_vecs.reset_index(drop = True).iloc[sorted_index]



type(sorted_ypu)
sorted_ypred = pd.DataFrame(sorted_ypu)
sorted_ypred.head(10)



out = pd.concat([sorted_items, sorted_y], axis=1)
out['prediction_score'] = sorted_ypred.values





out.head(30)








drama_vecs = item.drop_duplicates(subset=['drama_name'])


drama_vecs.info()





user.head()


new_user_vecs = user.iloc[0:1]


new_user_vecs = new_user_vecs*0


new_user_vecs.head()






new_user_vecs['Thriller'] = 9
new_user_vecs['Horror'] = 10
new_user_vecs['Supernatural'] = 10
new_user_vecs['Science-Fiction'] = 10








row_array = new_user_vecs.values
repeated_array = np.tile(row_array, (len(drama_vecs), 1))


new_user_vecs = pd.DataFrame(
    repeated_array, 
    columns=new_user_vecs.columns
)



new_user_vecs = new_user_vecs.drop(['user_id', 'overall_score'], axis = 1)





snuser_vecs = scalerUser.transform(new_user_vecs)
snitem_vecs = scalerItem.transform(drama_vecs.iloc[:,2:])


y_p = model.predict([snuser_vecs, snitem_vecs])

# unscale y prediction 
y_pu = scalerTarget.inverse_transform(y_p)



sorted_index = np.argsort(-y_pu,axis=0).reshape(-1).tolist()  
sorted_ypu   = y_pu[sorted_index]
sorted_ypu = pd.DataFrame(sorted_ypu)
sorted_items = drama_vecs.iloc[sorted_index] 


result = sorted_items.iloc[:,1:4]


result['predicted_ratings'] = sorted_ypu.values





drama_with_genre = pd.read_csv('data_preprocessing/korean_drama_complete.csv')


drama_with_genre.head()


merged_result = pd.merge(
    result, 
    drama_with_genre[['drama_name', 'Genre']], 
    on='drama_name', 
    how='left'
)


merged_result = merged_result[['drama_name', 'Genre', 'pop', 'rank', 'predicted_ratings']]





merged_result.head(50)





input_item_m = tf.keras.layers.Input(shape=(num_item_features))
vm_m = item_NN(input_item_m)                                       # use the trained item_NN
vm_m = tf.linalg.l2_normalize(vm_m, axis=1)                        
model_m = tf.keras.Model(input_item_m, vm_m)                                
model_m.summary()


drama_vecs.head()


scaled_item_vecs = scalerItem.transform(drama_vecs.iloc[:,2:])
vms = model_m.predict(scaled_item_vecs)
print(f"size of all predicted movie feature vectors: {vms.shape}")


drama_with_genre.head()





sorted_indices = drama_vecs['pop'].argsort() #sort dramas by popularity


vms = vms[sorted_indices] #vms stores drama feature vectors - shape(1104,32) 1104 dramas each has 32 feature vectors
drama_vecs = drama_vecs.iloc[sorted_indices].reset_index(drop=True)  #drama_vecs stores drama with their features. sorted accdording to pop

count = 50  # number of dramas to display
dim = len(vms)
dist = np.zeros((dim,dim))

for i in range(dim):      #calcualte square distance between dramas
    for j in range(dim):
        dist[i,j] = np.sum((vms[i,:] - vms [j, :])**2) #single j loop calculates dist between two dramas, a single i loop calculates dist between one drama and all the remaining dramas
                                                       #the distances between a single drama and all remaining dramas are stored in a single row.  
m_dist = ma.masked_array(dist, mask=np.identity(dist.shape[0]))  # mask the diagonal since the diagonals will have 0 values as they're the dist betn same drama

disp = [["movie1", "genre1", "movie2", "genre2"]]
for i in range(count):
    min_idx = np.argmin(m_dist[i])  #finds the index of the minimum number in ith row. 
    drama1_id = int(drama_vecs.iloc[i,0])
    drama2_id = int(drama_vecs.iloc[min_idx,0]) 
    name1 = drama_vecs[drama_vecs['kdrama_id'] == drama1_id]['drama_name'].iloc[0]
    name2 = drama_vecs[drama_vecs['kdrama_id'] == drama2_id]['drama_name'].iloc[0]
    
    genre1 = drama_with_genre[drama_with_genre['drama_name'] == name1]['Genre'].iloc[0]
    
    genre2 = drama_with_genre[drama_with_genre['drama_name'] == name2]['Genre'].iloc[0]

    disp.append([name1, genre1, name2, genre2])
table = tabulate.tabulate(disp, tablefmt='html', headers="firstrow")
table


def find_similar_dramas( n=5):

    drama_name = input("Enter Drama Name:").strip()
    
    
    if drama_name.lower() not in drama_vecs['drama_name'].str.lower().values:
        print(f"Drama '{drama_name}' not found!")
        return None
    
    
    drama_idx = drama_vecs[drama_vecs['drama_name'].str.lower() == drama_name.lower()].index[0]    
   
    similar_indices = np.argsort(m_dist[drama_idx])[:n] 
    
    
    print(f"\nTop {n} dramas similar to '{drama_name}':\n")
    for rank, idx in enumerate(similar_indices, 1):
        name = drama_vecs.iloc[idx]['drama_name']
        genre = drama_with_genre[drama_with_genre['drama_name'] == name]['Genre'].iloc[0]
        distance = m_dist[drama_idx][idx]
        print(f"{rank}. {name} ({genre}) - Distance: {distance:.4f}")





find_similar_dramas(10)



